{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sentences:\n",
      "141400\n",
      "selected sentences:\n",
      "141400\n",
      "percent selected for processeing:\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "## Each Space is a cell in a juptyr notebook (\"QuickThinking\" in fastai_workbench) that runs on Gradient Paperspace\n",
    "from greek_normalisation.utils import (nfd, nfc, nfkc, convert_to_2019)\n",
    "from greek_accentuation.characters import base\n",
    "from cltk.lemmatize.grc import GreekBackoffLemmatizer\n",
    "from alphabet import filter_non_greek, filter_non_greek_with_punctuation\n",
    "from alphabet import PUNCTUATION\n",
    "from cltk.stops.grc import STOPS\n",
    "from greek_swadesh import SWADESH\n",
    "from chrysostom_junk_words import JUNK_WORDS\n",
    "import numpy as np\n",
    "\n",
    "#greek_text = 'ὁ αὐτὸς μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος; \\nκαὶ κατέβην χθὲς εἰς Πειραιᾶ μετὰ Γλαύκωνος τοῦ Ἀρίστωνος. Τιμταμ;'\n",
    "file = open('/notebooks/storage/greek_text/ALL_REAL_JOHN_FOR_BAG_OF_WORDS.TXT')\n",
    "greek_text = file.read().lower()\n",
    "file.close()\n",
    "all_text = greek_text.split('.')\n",
    "print(\"total sentences:\")\n",
    "total_sentences = len(all_text)\n",
    "print(total_sentences)\n",
    "#no_of_sentences = total_sentences * 0.25\n",
    "#no_of_sentences = total_sentences * 0.50\n",
    "#no_of_sentences = total_sentences * 0.75\n",
    "no_of_sentences = total_sentences * 1\n",
    "selection = all_text[0:int(no_of_sentences)] #grab only the first XXXX sentences for testing\n",
    "print(\"selected sentences:\")\n",
    "print(len(selection))\n",
    "percent_selected = (len(selection) / len(all_text) * 100)\n",
    "print(\"percent selected for processeing:\")\n",
    "print(percent_selected)\n",
    "greek_text = '.'.join(selection)\n",
    "original_text = greek_text\n",
    "lemmatizer = GreekBackoffLemmatizer()\n",
    "\n",
    "def strip_diacriticals(greek_text):\n",
    "    return ''.join([base(char) for char in greek_text])\n",
    "\n",
    "def normalize_greek(greek_text):\n",
    "    greek_text = nfc(greek_text) #normalize pre-composed unicode characters to be single characters\n",
    "    greek_text = convert_to_2019(greek_text).lower() #normalize apostrophes and lowercase\n",
    "    greek_text = filter_non_greek_with_punctuation(greek_text) #strip everything but greek and basic punctuation\n",
    "    greek_text = greek_text.replace(';','.').replace('·','.') #replace all punctuation except \",\"s with \".\"s\n",
    "    return greek_text\n",
    "\n",
    "def unhelpful_words():\n",
    "    stops = sorted(normalize_greek(' '.join(STOPS)).split(' '))\n",
    "    swadesh = sorted(normalize_greek(' '.join(SWADESH)).split(' '))\n",
    "    empty = ['', ' ', '\\n']\n",
    "    common = ['ἄνευ','εισ','ὡσανεὶ','θ','ταῦτ','πρό','δήποτε', 'δεύτερος', 'αἰών','τοσοῦτος', 'ποτέ','ἴσως', 'ἐκεῖνος','ἐπεί','ἀνά','εἴτε','φησὶ,','ἰδοὺ', 'εἶπε,', 'πῶς,', 'τοσοῦτον,', 'ὅσος','μηδείς', 'μηδέ', 'μήν','γὰρ,', 'εἶπον','ἐστὶν,','ἐστὶν', 'ὅτε','ποῖος','μοι,','ἐρῶ','τοῦτο,','ἀντί','πόσος','ἐστιν','εἶπεν','εὐθύς','ποῦ','ἔμπροσθεν','ἀπαιτέω','κἀν','διό' 'ποτέ' 'ἵημι' 'ἡμῶν,' 'σαφής' 'ὅπου', 'γίγνομαι', 'ἵημι', 'ἐντεῦθεν','ἕνεκα','σου','δοκέω','φησὶ', 'φημί','τίθημι','γὰρ','τοσοῦτος','ὅσπερ','πᾶς','μόνος','ὅταν','ἅπας','δύναμαι','καθά','τοσοῦτος','ἀρετή','ἡμέτερος','ἀεί','τουτέστι,','πλῆθος','αὐτὸν', 'εἶναι', 'ἦν', 'ἐστιν,' 'ἐπειδὴ', 'αὐτῷ', 'αὐτοὺς', 'τότε', 'τούτου', 'πάντες', 'αὕτη', 'ἦν', 'πάντας', 'ὅμως', 'φησιν', 'τουτέστι', 'ἔχειν', 'πλέον', 'ἑαυτοὺς', 'οὕτω', 'οὐχ', 'ὥσπερ', 'ἐστι', 'ἐστιν', 'μόνον', 'τοῦτό', 'ἔχει', 'ὑμᾶς', 'ἐστὶ', 'μέγα', 'γῆς', 'ποιεῖν', 'τοσοῦτον', 'ὅσον', 'μικρὸν', 'γοῦν', 'γέγονε', 'ἵνα', 'ἡμᾶς', 'ἡμῶν', 'σου', 'σε', 'εἶπεν', 'με', 'πᾶσαν', 'κατ', 'ἐκεῖνο', 'εἶπεν', 'αὐτῇ', 'γένοιτο', 'καθάπερ', 'ἔστι', 'ἐστιν', 'ὑμῶν', 'ἕτερον', 'οὗ', 'ἀεὶ',  'οἷς', 'λέγοντος', 'ἐμοὶ', 'νῦν', 'ταῦτα', 'πάντα', 'μόνον', 'ἡμῖν', 'τούτων', 'πάντων', 'οὐχὶ', 'ἐκείνων', 'μηδὲν', 'δὲ', 'τοῦτον', 'ἐστὶν',  'ἐκεῖνα', 'ἆρα', 'μετ', 'λέγων', 'α', 'ἄλλως', 'οὐδὲν', 'φησὶ', 'πάλιν', 'αὐτοῖς', 'εἶτα', 'πολλὰ', 'καλῶς', 'ἃ', 'τοιοῦτον', 'ἐκεῖνοι', 'εἶναι', 'χρὴ', 'μείζονα', 'τουτέστιν', 'αὑτοῦ', 'πάντοθεν', 'αἰτίαν', 'τοῦτο', 'τὰς', 'αἱ', 'μηδὲ', 'αὐτὴν', 'καίτοι', 'ἄλλων', 'ποιεῖ', 'γενέσθαι', 'ποτε', 'εἶπε', 'παντὸς', 'ἐκείνην', 'οἷον', 'ἀλλὰ', 'ἕκαστος', 'εἰπεῖν', 'φησὶν', 'δι', 'τοίνυν', 'ταύτην', 'πάλιν', 'ὧν', 'σφόδρα', 'σοι', 'εὐθέως', 'δῆλον', 'πολλὴν', 'ἕως', 'πόθεν', 'ἤδη', 'οὖν', 'εἰσιν', 'τὸ', 'οὐδέν', 'δεῖ', 'γὰρ', 'ὃ', 'λέγων', 'μου', 'τούτῳ', 'ἐπεὶ', 'πρῶτον', 'λέγει', 'οὐκοῦν', 'αὐτοὶ', 'οὗτοι', 'γ', 'αὐτόν', 'φησι', 'ταῖς', 'ὅπερ', 'ὁρᾷς', 'αὐτῆς', 'ὄντως', 'ἐκείνου', 'μήτε', 'μοι', 'διὸ', 'χωρὶς', 'ἅμα', 'εἰπέ', 'εἰπέ_μοι', 'ἔσται', 'ὁμοίως', 'ἐκείνῳ',  'αὐτοῦ', 'αὐτῶν', 'παρ', 'γίνεται', 'λέγω', 'οὐκέτι', 'τοιαῦτα', 'εἰκότως', 'αὐτοῦ', 'ἑτέρων', 'β', 'ἐκείνης', 'μὴν', 'πολλῆς', 'πολλάκις', 'ἁπλῶς', 'ἕνεκεν', 'αὐτὰ', 'ὑμῖν', 'τίνος', 'πάσης', 'φησι', 'μυρία', 'τίνος_ἕνεκεν', 'ἔχων', 'ἧς', 'νῦν', 'αὐτὸ', 'ἔστιν', 'φησίν', 'καθ', 'λέγει', 'μέχρι', 'πρὸ', 'πᾶν', 'τοῦτο', 'ἐκεῖνον', 'ἅπερ', 'πολλοὶ', 'ἄλλο', 'ἦσαν', 'ἔλεγεν', 'ἀμήν', 'μοι', 'μάλιστα', 'ταύτης', 'ἐστι', 'πανταχοῦ', 'φησί', 'λοιπὸν', 'εἰπεῖν', 'τινες', 'πολὺ', 'ἐστίν', 'ἅπαντα', 'ἀντὶ', 'οὐδαμοῦ', 'ἀλλήλους', 'μᾶλλον', 'κἂν', 'πολλῷ', 'εἴ', 'πολλῷ', 'μᾶλλον', 'νόμον', 'ἔνθα', 'ᾖ', 'τούτοις', 'ταχέως', 'εἶδες', 'ὅλως', 'οὐδέποτε']\n",
    "    return sorted(list(set(np.concatenate((stops, swadesh, empty, common)))))\n",
    "\n",
    "UNHELPFUL = unhelpful_words()\n",
    "#print(UNHELPFUL)\n",
    "\n",
    "def remove_words_unhelpful_for_lda(lemmata):\n",
    "    helpful_words = [word for word in lemmata if word not in UNHELPFUL and word != '' and word != None]\n",
    "    return helpful_words\n",
    "\n",
    "def tokenize(sentence):\n",
    "    sentence = sentence.replace(',','').split(' ')\n",
    "    lemmatized_tuples = lemmatizer.lemmatize(sentence)\n",
    "    lemmata = [tuple[1] for tuple in lemmatized_tuples]\n",
    "    lemmata = remove_words_unhelpful_for_lda(lemmata)\n",
    "    return lemmata\n",
    "\n",
    "def make_lda_documents(greek_text):\n",
    "    # LDA wants a List of Lists of words (= document), I will make each sentence a document\n",
    "    print(\"chars in greek text:\")\n",
    "    print(len(greek_text))\n",
    "    print(\"words in greek text:\")\n",
    "    print(len(greek_text.split(' ')))\n",
    "\n",
    "    print(\"normalizing greek...\")\n",
    "    import datetime\n",
    "    start_normalizing = datetime.datetime.now()\n",
    "    print(start_normalizing)\n",
    "    documents = normalize_greek(greek_text).split('.')\n",
    "    print(\"...done.\")\n",
    "    done_normalizing = datetime.datetime.now()\n",
    "    print(done_normalizing)\n",
    "\n",
    "    print(\"tokenizing...\")\n",
    "    start_tokenizing = datetime.datetime.now()\n",
    "    print(start_tokenizing)\n",
    "\n",
    "    lda_documents = []\n",
    "    sentences_per_document = 50 # doc ~ a paragraph / thought\n",
    "    current_document = []\n",
    "\n",
    "    for sentence in documents:\n",
    "        if len(sentence) > 0:\n",
    "            sentence = tokenize(sentence)\n",
    "        if len(sentence) > 0:\n",
    "            current_document.extend(sentence)\n",
    "            if len(current_document) >= sentences_per_document:\n",
    "                lda_documents.append(current_document)\n",
    "                current_document = []\n",
    "\n",
    "    print(\"...done.\")\n",
    "    done_tokenizing = datetime.datetime.now()\n",
    "    print(done_tokenizing)\n",
    "\n",
    "    print(\"number of lsa_documents:\")\n",
    "    print(len(lda_documents))\n",
    "\n",
    "    print(\"peek at lsa_documents:\")\n",
    "    print(lda_documents[-3])\n",
    "\n",
    "\n",
    "    return lda_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars in greek text:\n",
      "21049531\n",
      "words in greek text:\n",
      "3328304\n",
      "normalizing greek...\n",
      "2020-10-13 13:37:27.974069\n"
     ]
    }
   ],
   "source": [
    "#Run Preprocessing\n",
    "lda_documents = make_lda_documents(greek_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute bigrams\n",
    "from gensim.models import Phrases\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(lda_documents, min_count=20)\n",
    "for idx in range(len(lda_documents)):\n",
    "    for token in bigram[lda_documents[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            lda_documents[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rare and common tokens (Filter out words that occur less than 20 documents, or more than 50% of the documents)\n",
    "from gensim.corpora import Dictionary\n",
    "lda_dictionary = Dictionary(lda_documents)\n",
    "lda_dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize documents into Bag-of-words.\n",
    "lda_corpus = [lda_dictionary.doc2bow(doc) for doc in lda_documents]\n",
    "#Let’s see how many tokens and documents we have to train on.\n",
    "print('Number of unique tokens: %d' % len(lda_dictionary))\n",
    "print('Number of documents: %d' % len(lda_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Train LDA model.\n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 50 #15\n",
    "chunksize = 1500\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = lda_dictionary[0]  # This is only to \"load\" the lda_dictionary.\n",
    "id2word = lda_dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=lda_corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "top_topics = model.top_topics(lda_corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
