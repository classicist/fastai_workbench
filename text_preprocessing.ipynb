{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Each Space is a cell in a juptyr notebook (\"QuickThinking\" in fastai_workbench) that runs on Gradient Paperspace\n",
    "from greek_normalisation.utils import (nfd, nfc, nfkc, convert_to_2019)\n",
    "from greek_accentuation.characters import base\n",
    "from cltk.lemmatize.grc import GreekBackoffLemmatizer\n",
    "from alphabet import filter_non_greek, filter_non_greek_with_punctuation\n",
    "from alphabet import PUNCTUATION\n",
    "from cltk.stops.grc import STOPS\n",
    "from greek_swadesh import SWADESH\n",
    "from chrysostom_junk_words import JUNK_WORDS\n",
    "import numpy as np\n",
    "import datetime\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import sys\n",
    "class TextDataSet:\n",
    "    def __init__(self, file_path = \"default_text\", verbose = True):\n",
    "        self.file_path = file_path\n",
    "        self.verbose = verbose\n",
    "        self.mode_dataset_percentages = {'development': 0.05,'exploration': 0.25, 'training': 0.70, 'testing': 0.90, 'validation': 1.0}\n",
    "\n",
    "        if self.file_path != \"default_text\":\n",
    "            file = open(file_path)\n",
    "            self.text = file.read()\n",
    "            file.close()\n",
    "        else:\n",
    "            self.text = 'ὁ αὐτὸς μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος; \\nκαὶ κατέβην χθὲς εἰς Πειραιᾶ μετὰ Γλαύκωνος τοῦ Ἀρίστωνος. Τιμταμ;'\n",
    "\n",
    "        self.all_documents = self.text.split('.') #Defer: pass in a lambda to split into docs, if I need it\n",
    "        self.number_of_documents = len(self.all_documents)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"segmented text into documents from: \" + self.file_path)\n",
    "            print(\"loaded \" + str(self.number_of_documents) + \" total documents\")\n",
    "\n",
    "    def __texts_for_mode(self, current_mode, prior_mode):\n",
    "        self.start_index = self.__start_index(current_mode, prior_mode)\n",
    "        self.end_index   = self.__end_index(current_mode)\n",
    "        documents        = self.all_documents[self.start_index:self.end_index]\n",
    "        greek_text = '.'.join(documents)\n",
    "        if self.verbose:\n",
    "            print(\"selected document range for: \" + current_mode)\n",
    "            print(\"selected \" + str(len(documents)) + \" documents from indexes: \" + str(self.start_index) +':'+ str(self.end_index))\n",
    "            print(\"percent selected for processeing: \" + str(self.mode_dataset_percentages[current_mode] * 100))            \n",
    "        return greek_text\n",
    "\n",
    "    def __start_index(self, current_mode, prior_mode):\n",
    "        if prior_mode == None:\n",
    "            start_index = 0\n",
    "        else:\n",
    "            prior_document_end_index = self.number_of_documents * self.mode_dataset_percentages[prior_mode]\n",
    "            start_index = prior_document_end_index + 1\n",
    "        return int(start_index)\n",
    "     \n",
    "    def __end_index(self, current_mode):\n",
    "        return int(self.number_of_documents * self.mode_dataset_percentages[current_mode])\n",
    "\n",
    "    def development(self):\n",
    "        self.mode = sys._getframe().f_code.co_name\n",
    "        return self.__texts_for_mode(self.mode, None)\n",
    "    \n",
    "    def exploration(self):\n",
    "        self.mode = sys._getframe().f_code.co_name\n",
    "        return self.__texts_for_mode(self.mode, None)\n",
    "\n",
    "    def training(self):\n",
    "        self.mode = sys._getframe().f_code.co_name\n",
    "        return self.__texts_for_mode(self.mode, None)\n",
    "\n",
    "    def testing(self):\n",
    "        self.mode = sys._getframe().f_code.co_name\n",
    "        return self.__texts_for_mode(self.mode, 'training')\n",
    "\n",
    "    def validation(self):\n",
    "        self.mode = sys._getframe().f_code.co_name\n",
    "        return self.__texts_for_mode(self.mode, 'testing')\n",
    "\n",
    "class GreekPreprocessor:\n",
    "    def __init__(self, lemmatizer_class = GreekBackoffLemmatizer, verbose = True):\n",
    "        self.build_unhelpful_word_list()\n",
    "        self.lemmatizer = lemmatizer_class()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def build_unhelpful_word_list(self):\n",
    "        stops = sorted(self.normalize_greek(' '.join(STOPS)).split(' '))\n",
    "        swadesh = sorted(self.normalize_greek(' '.join(SWADESH)).split(' '))\n",
    "        empty = ['', ' ', '\\n']\n",
    "        common = ['ἄνευ','εισ','ὡσανεὶ','θ','ταῦτ','πρό','δήποτε', 'δεύτερος', 'αἰών','τοσοῦτος', 'ποτέ','ἴσως', 'ἐκεῖνος','ἐπεί','ἀνά','εἴτε','φησὶ,','ἰδοὺ', 'εἶπε,', 'πῶς,', 'τοσοῦτον,', 'ὅσος','μηδείς', 'μηδέ', 'μήν','γὰρ,', 'εἶπον','ἐστὶν,','ἐστὶν', 'ὅτε','ποῖος','μοι,','ἐρῶ','τοῦτο,','ἀντί','πόσος','ἐστιν','εἶπεν','εὐθύς','ποῦ','ἔμπροσθεν','ἀπαιτέω','κἀν','διό' 'ποτέ' 'ἵημι' 'ἡμῶν,' 'σαφής' 'ὅπου', 'γίγνομαι', 'ἵημι', 'ἐντεῦθεν','ἕνεκα','σου','δοκέω','φησὶ', 'φημί','τίθημι','γὰρ','τοσοῦτος','ὅσπερ','πᾶς','μόνος','ὅταν','ἅπας','δύναμαι','καθά','τοσοῦτος','ἀρετή','ἡμέτερος','ἀεί','τουτέστι,','πλῆθος','αὐτὸν', 'εἶναι', 'ἦν', 'ἐστιν,' 'ἐπειδὴ', 'αὐτῷ', 'αὐτοὺς', 'τότε', 'τούτου', 'πάντες', 'αὕτη', 'ἦν', 'πάντας', 'ὅμως', 'φησιν', 'τουτέστι', 'ἔχειν', 'πλέον', 'ἑαυτοὺς', 'οὕτω', 'οὐχ', 'ὥσπερ', 'ἐστι', 'ἐστιν', 'μόνον', 'τοῦτό', 'ἔχει', 'ὑμᾶς', 'ἐστὶ', 'μέγα', 'γῆς', 'ποιεῖν', 'τοσοῦτον', 'ὅσον', 'μικρὸν', 'γοῦν', 'γέγονε', 'ἵνα', 'ἡμᾶς', 'ἡμῶν', 'σου', 'σε', 'εἶπεν', 'με', 'πᾶσαν', 'κατ', 'ἐκεῖνο', 'εἶπεν', 'αὐτῇ', 'γένοιτο', 'καθάπερ', 'ἔστι', 'ἐστιν', 'ὑμῶν', 'ἕτερον', 'οὗ', 'ἀεὶ',  'οἷς', 'λέγοντος', 'ἐμοὶ', 'νῦν', 'ταῦτα', 'πάντα', 'μόνον', 'ἡμῖν', 'τούτων', 'πάντων', 'οὐχὶ', 'ἐκείνων', 'μηδὲν', 'δὲ', 'τοῦτον', 'ἐστὶν',  'ἐκεῖνα', 'ἆρα', 'μετ', 'λέγων', 'α', 'ἄλλως', 'οὐδὲν', 'φησὶ', 'πάλιν', 'αὐτοῖς', 'εἶτα', 'πολλὰ', 'καλῶς', 'ἃ', 'τοιοῦτον', 'ἐκεῖνοι', 'εἶναι', 'χρὴ', 'μείζονα', 'τουτέστιν', 'αὑτοῦ', 'πάντοθεν', 'αἰτίαν', 'τοῦτο', 'τὰς', 'αἱ', 'μηδὲ', 'αὐτὴν', 'καίτοι', 'ἄλλων', 'ποιεῖ', 'γενέσθαι', 'ποτε', 'εἶπε', 'παντὸς', 'ἐκείνην', 'οἷον', 'ἀλλὰ', 'ἕκαστος', 'εἰπεῖν', 'φησὶν', 'δι', 'τοίνυν', 'ταύτην', 'πάλιν', 'ὧν', 'σφόδρα', 'σοι', 'εὐθέως', 'δῆλον', 'πολλὴν', 'ἕως', 'πόθεν', 'ἤδη', 'οὖν', 'εἰσιν', 'τὸ', 'οὐδέν', 'δεῖ', 'γὰρ', 'ὃ', 'λέγων', 'μου', 'τούτῳ', 'ἐπεὶ', 'πρῶτον', 'λέγει', 'οὐκοῦν', 'αὐτοὶ', 'οὗτοι', 'γ', 'αὐτόν', 'φησι', 'ταῖς', 'ὅπερ', 'ὁρᾷς', 'αὐτῆς', 'ὄντως', 'ἐκείνου', 'μήτε', 'μοι', 'διὸ', 'χωρὶς', 'ἅμα', 'εἰπέ', 'εἰπέ_μοι', 'ἔσται', 'ὁμοίως', 'ἐκείνῳ',  'αὐτοῦ', 'αὐτῶν', 'παρ', 'γίνεται', 'λέγω', 'οὐκέτι', 'τοιαῦτα', 'εἰκότως', 'αὐτοῦ', 'ἑτέρων', 'β', 'ἐκείνης', 'μὴν', 'πολλῆς', 'πολλάκις', 'ἁπλῶς', 'ἕνεκεν', 'αὐτὰ', 'ὑμῖν', 'τίνος', 'πάσης', 'φησι', 'μυρία', 'τίνος_ἕνεκεν', 'ἔχων', 'ἧς', 'νῦν', 'αὐτὸ', 'ἔστιν', 'φησίν', 'καθ', 'λέγει', 'μέχρι', 'πρὸ', 'πᾶν', 'τοῦτο', 'ἐκεῖνον', 'ἅπερ', 'πολλοὶ', 'ἄλλο', 'ἦσαν', 'ἔλεγεν', 'ἀμήν', 'μοι', 'μάλιστα', 'ταύτης', 'ἐστι', 'πανταχοῦ', 'φησί', 'λοιπὸν', 'εἰπεῖν', 'τινες', 'πολὺ', 'ἐστίν', 'ἅπαντα', 'ἀντὶ', 'οὐδαμοῦ', 'ἀλλήλους', 'μᾶλλον', 'κἂν', 'πολλῷ', 'εἴ', 'πολλῷ', 'μᾶλλον', 'νόμον', 'ἔνθα', 'ᾖ', 'τούτοις', 'ταχέως', 'εἶδες', 'ὅλως', 'οὐδέποτε']\n",
    "        self.unhelpful_words = sorted(list(set(np.concatenate((stops, swadesh, empty, common)))))\n",
    "        return self.unhelpful_words\n",
    "    \n",
    "    def strip_diacriticals(self, greek_text):\n",
    "        return ''.join([base(char) for char in greek_text])\n",
    "\n",
    "    def normalize_greek(self, greek_text):\n",
    "        greek_text = nfc(greek_text) #normalize pre-composed unicode characters to be single characters\n",
    "        print(\"nfc-ed\")\n",
    "        greek_text = convert_to_2019(greek_text).lower() #normalize apostrophes and lowercase\n",
    "        print(\"converted_to_2019\")\n",
    "        \n",
    "        # Run out of Memory here for with Training Mode data:\n",
    "        greek_text = filter_non_greek_with_punctuation(greek_text) #strip everything but greek and basic punctuation\n",
    "        print(\"filtered_non_greek\")\n",
    "        greek_text = greek_text.replace(';','.').replace('·','.') #replace all punctuation except \",\"s with \".\"s\n",
    "        print(\"replaced punctuation\")\n",
    "        return greek_text\n",
    "\n",
    "    def remove_words_unhelpful_for_lda(self, lemmata):\n",
    "        helpful_words = [word for word in lemmata if word not in self.unhelpful_words and word != '' and word != None]\n",
    "        return helpful_words\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        sentence = sentence.replace(',','').split(' ')\n",
    "        lemmatized_tuples = self.lemmatizer.lemmatize(sentence)\n",
    "        lemmata = [tuple[1] for tuple in lemmatized_tuples]\n",
    "        lemmata = self.remove_words_unhelpful_for_lda(lemmata)\n",
    "        return lemmata\n",
    "\n",
    "    def make_lda_documents(self, greek_text):\n",
    "        # LDA wants a List of Lists of words (= document), I will make each sentence a document\n",
    "        if self.verbose:\n",
    "            print(\"chars in greek text:\")\n",
    "            print(len(greek_text))\n",
    "            print(\"words in greek text:\")\n",
    "            print(len(greek_text.split(' ')))\n",
    "            print(\"normalizing greek...\")\n",
    "            start_normalizing = datetime.datetime.now()\n",
    "            print(start_normalizing)\n",
    "        \n",
    "        documents = self.normalize_greek(greek_text).split('.')\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"...done.\")\n",
    "            done_normalizing = datetime.datetime.now()\n",
    "            print(done_normalizing)\n",
    "            print(\"tokenizing...\")\n",
    "            start_tokenizing = datetime.datetime.now()\n",
    "            print(start_tokenizing)\n",
    "\n",
    "        lda_documents = []\n",
    "        sentences_per_document = 50 # doc ~ a paragraph / thought\n",
    "        current_document = []\n",
    "\n",
    "        for sentence in documents:\n",
    "            if len(sentence) > 0:\n",
    "                sentence = self.tokenize(sentence)\n",
    "            if len(sentence) > 0:\n",
    "                current_document.extend(sentence)\n",
    "                if len(current_document) >= sentences_per_document:\n",
    "                    lda_documents.append(current_document)\n",
    "                    current_document = []\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"...done.\")\n",
    "            done_tokenizing = datetime.datetime.now()\n",
    "            print(done_tokenizing)\n",
    "            print(\"number of lda_documents:\")\n",
    "            print(len(lda_documents))\n",
    "            print(\"peek at lda_documents:\")\n",
    "            print(lda_documents[-3])\n",
    "\n",
    "        return lda_documents\n",
    "\n",
    "    def add_bigrams(self, documents, min_count=20):\n",
    "        # Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "        bigram = Phrases(documents, min_count=min_count)\n",
    "        for idx in range(len(documents)):\n",
    "            for token in bigram[documents[idx]]:\n",
    "                if '_' in token:\n",
    "                    # Token is a bigram, add to document.\n",
    "                    documents[idx].append(token)\n",
    "        return documents\n",
    "    \n",
    "    def make_dictionary(self, documents, no_below=20, no_above=0.5):\n",
    "        #Remove rare and common tokens (Filter out words that occur less than 20 documents, or more than 50% of the documents)\n",
    "        dictionary = Dictionary(documents)\n",
    "        dictionary.filter_extremes(no_below=no_below, no_above=no_below)\n",
    "        return dictionary\n",
    "    \n",
    "    def make_bag_of_words_corpus(self, dictionary, documents):\n",
    "        #Vectorize documents into Bag-of-words.\n",
    "        bag_of_words_corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "        if self.verbose:\n",
    "            #Let’s see how many tokens and documents we have to train on.\n",
    "            print('Number of unique tokens: %d' % len(dictionary))\n",
    "            print('Number of documents: %d' % len(bag_of_words_corpus))\n",
    "        return bag_of_words_corpus\n",
    "    \n",
    "    def lda_corpus_factory(self, greek_text):\n",
    "        lda_documents = self.make_lda_documents(greek_text)\n",
    "        lda_documents = self.add_bigrams(lda_documents)\n",
    "        dictionary    = self.make_dictionary(lda_documents)\n",
    "        corpus        = self.make_bag_of_words_corpus(dictionary, lda_documents)\n",
    "        return(corpus, dictionary, lda_documents)\n",
    "    \n",
    "    def word2vec_corpus_factory(self, greek_text):\n",
    "        lda_documents = self.make_lda_documents(greek_text)\n",
    "        lda_documents = self.add_bigrams(lda_documents)\n",
    "        return lda_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmented text into documents from: /notebooks/storage/greek_text/ALL_REAL_JOHN_FOR_BAG_OF_WORDS.TXT\n",
      "loaded 141400 total documents\n",
      "selected document range for: training\n",
      "selected 98980 documents from indexes: 0:98980\n",
      "percent selected for processeing: 70.0\n",
      "nfc-ed\n",
      "converted_to_2019\n",
      "filtered_non_greek\n",
      "replaced punctuation\n",
      "nfc-ed\n",
      "converted_to_2019\n",
      "filtered_non_greek\n",
      "replaced punctuation\n",
      "chars in greek text:\n",
      "15225445\n",
      "words in greek text:\n",
      "2405150\n",
      "normalizing greek...\n",
      "2020-10-14 14:22:13.972572\n",
      "nfc-ed\n",
      "converted_to_2019\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "path = '/notebooks/storage/greek_text/ALL_REAL_JOHN_FOR_BAG_OF_WORDS.TXT'\n",
    "greek_text = TextDataSet(path).training()\n",
    "preprocessor  = GreekPreprocessor()\n",
    "documents = preprocessor.word2vec_corpus_factory(greek_text)\n",
    "\n",
    "#Train Word2Vec model\n",
    "#https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-download-auto-examples-tutorials-run-word2vec-py\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=documents,\n",
    "    min_count=10,\n",
    "    size=200,\n",
    "    workers=4, #requires Cython to be installed to have any effect. Othewise you are stuck on 1 core.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_model.most_similar_cosmul(nfc('λόγος')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "path = '/notebooks/storage/greek_text/ALL_REAL_JOHN_FOR_BAG_OF_WORDS.TXT'\n",
    "greek_text = TextDataSet(path).training()\n",
    "preprocessor  = GreekPreprocessor()\n",
    "lda_corpus, lda_dictionary, lda_documents = preprocessor.lda_corpus_factory(greek_text)\n",
    "\n",
    "# Train LDA model.\n",
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-auto-examples-tutorials-run-lda-py\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 50 #15\n",
    "chunksize = 1500\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = lda_dictionary[0]  # This is only to \"load\" the lda_dictionary.\n",
    "id2word = lda_dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus = lda_corpus,\n",
    "    id2word = id2word,\n",
    "    chunksize = chunksize,\n",
    "    alpha = 'auto',\n",
    "    eta = 'auto',\n",
    "    iterations = iterations,\n",
    "    num_topics = num_topics,\n",
    "    passes = passes,\n",
    "    eval_every = eval_every\n",
    ")\n",
    "\n",
    "top_topics = model.top_topics(lda_corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
